<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="../styles.css">
    

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">

    
    <!-- Rubik font -->
    <link
      href="https://fonts.googleapis.com/css2?family=Rubik&display=swap"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

    <script src="../loader.js"></script>

    <script src="https://kit.fontawesome.com/9016fec2c3.js" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/showdown@2.1.0/dist/showdown.min.js"></script>

    <script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js?autorun=false"></script>

    <link rel="apple-touch-icon" sizes="180x180" href="../assets/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../assets/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../assets/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../assets/favicon_io/site.webmanifest" crossorigin="use-credentials">

    <title>A small collection</title>
  </head>
  <body>
    <div class="container col-sm" id="site-header">
    </div>
    <div class="container row" id="shareable-box">
      <div class="col">
        <a id="shareable-link" href="javascript:void(0);"><i class="fa-solid fa-link"></i> Copy shareable link for this write-up</a>
      </div>
    </div>
    <div class="row" id="content-block">
      <div class="col-xs" id="back-button">
        <a href="../index.html"><i class="fa-solid fa-chevron-left"></i></a>
      </div>
      <div class="container float-left col" id="post-block" style="font-family: 'Rubik', sans-serif;">
          <h1 id="quantizedlora">Quantized LoRA</h1>
<p>QLoRA is an attempt to further optimize finetuning of LLMs by doing it directly on quantized LLMs and letting the gradient updates happen to the LoRA adapters.</p>
<h3 id="quickoverviewoflora">Quick overview of LoRA</h3>
<p>LoRA is a technique to perform finetuning of LLMs without having to perform gradient updates on the entire parameter set (rather a much, much smaller subset).</p>
<p>It makes use of the assumption that the change of weights matrix ($\Delta W$) for each layer in an LLM has a low rank. Formally, if a weight matrix in an LLM is $W_{a \times b}$,</p>
<p>\[rank(\Delta W_{a \times b}) \ll a, b\]</p>
<p>This can be exploited by creating smaller matrices $A_{a \times h}$ and $B_{h \times b}$ (where $h \ll a, b$) that act as adapters for gradients to flow through them. The output of the layer then can be calculated as -</p>
<p>\[Y = XW + XAB\]</p>
<p>Where the weights in W are kept frozen, while A and B are updated.</p>
<h3 id="quickoverviewofquantization">Quick overview of Quantization</h3>
<p>With Quantization, we attempt to reduce the precision required per parameter value to save memory/speed up calculations, without sacrificing the information per bit too much. This is generally done through scaling floats within a discretized range, while taking care of outliers by doing it in a block-wise fashion. Not going into the details here, link attached below.</p>
<p>Quantization significantly speeds up inference of models and makes them more accessible to a wide range of machines (even raspberry pis somehow). However finetuning a quantized LLM results in a significantly decreased performance compared to finetuning pre-quantization (which is obviously more expensive and inaccessible).</p>
<h3 id="qlora">QLoRA</h3>
<p>Quantized LLMs can be combined with LoRA adapters ðŸ¥³</p>
<p>Essentially, QLoRA uses 2 precision types - 4 bit for storage (general transformer weights) and 16 bit for computation (through dequantization). 
The gradients are only computed for the LoRA adapters which use 16 bit always. Computationally - </p>
<p>\[Y^{16} = X^{16} dequant'(W^4) + X^{16} A^{16} B^{16}\]</p>
<p>Where the superscripts represent the representation. (The $dequant'$ function is not the standard dequantization, and is explained further on here.)</p>
<h3 id="tricksused">Tricks used</h3>
<p>The paper highlights some tricks and optimizations used for preserving memory and improving precision.</p>
<p>[TODO]</p>
      </div>
      <div id="not-mobile" class="col">
      </div>
    </div>

    <!-- Bootstrap JS (optional) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
  </body>
</html>
