<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="../styles.css">
    <meta name="google-site-verification" content="LM4GhB5Y1CsiX4EhmQygTx1arB_gKDc0wQjpK2MFTOs" />

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">

    
    <!-- Rubik font -->
    <link
      href="https://fonts.googleapis.com/css2?family=Rubik&display=swap"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

    <script src="../loader.js"></script>

    <script src="https://kit.fontawesome.com/9016fec2c3.js" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/showdown@2.1.0/dist/showdown.min.js"></script>

    <script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js?autorun=false"></script>

    <link rel="apple-touch-icon" sizes="180x180" href="../assets/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../assets/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../assets/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../assets/favicon_io/site.webmanifest" crossorigin="use-credentials">

    <title>A small collection</title>
  </head>
  <body>
    <div class="container col-sm" id="site-header">
    </div>
    <div class="container row" id="shareable-box">
      <div class="col">
        <a id="shareable-link" href="javascript:void(0);"><i class="fa-solid fa-link"></i> Copy shareable link for this write-up</a>
      </div>
    </div>
    <div class="row" id="content-block">
      <div class="col-xs" id="back-button">
        <a href="../index.html"><i class="fa-solid fa-chevron-left"></i></a>
      </div>
      <div class="container float-left col" id="post-block" style="font-family: 'Rubik', sans-serif;">
          <h1 id="mmapandtheconfusionwithllamassparsity"><code>mmap()</code> and the confusion with LLaMa's sparsity</h1>
<p>A recent article on <a href="https://news.ycombinator.com/item?id=35393284&ref=emergentmind" target="_blank">HN</a> blew up claiming the usage of <code>mmap()</code> had caused a memory usage reduction for LLaMa (30B param variant large language model) inference, and now it was possible to do it with 6GB RAM instead of $\ge$ 30GB to load weights and activations. Even quantized to 4-bit integers, it would still require $\ge$ 17GB to fully load the model, but maybe loading it fully isn't necessary? (it actually is)</p>
<h3 id="mmap"><code>mmap()</code></h3>
<p>It is used to create a mapping in the virtual address space of the process that calls it. This mapping can be to a file or a device, and they might require far more memory than is available, which is allotted to them in the virtual space. The OS handles the memory accesses on demand, using swap space and caching.</p>
<p>Using <code>mmap()</code> for loading LLaMa 30B resulted in a reported memory consumption of only 6GB through htop, which was largely attributed to sparsity within the weights.</p>
<h3 id="matrixmultiplication">Matrix multiplication</h3>
<p>In reality, htop seemed to be lacking a reporting of filesystem cache where the entire LLaMa was still being accessed through OS calls.</p>
<ol>
<li><p>It is not possible to fix sparsity through <code>mmap()</code>. Transformers are a bunch of stacked matrix multiplications, and you cannot access parts of matrices and multiply them fully.</p></li>
<li><p>The sparsity claim itself lacks <a href="https://github.com/ggerganov/llama.cpp/discussions/638#discussioncomment-5494574" target="_blank">ground</a>.</p></li>
</ol>
<p>Is it beneficial at all then to use <code>mmap()</code>? Turns out yes, since now the model isn't being copied over to memory. Model loading is faster, and subsequent runs make use of caching through OS. However the inference time takes a massive hit due to constant disk reads.</p>
      </div>
      <div id="not-mobile" class="col">
      </div>
    </div>

    <!-- Bootstrap JS (optional) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
  </body>
</html>
