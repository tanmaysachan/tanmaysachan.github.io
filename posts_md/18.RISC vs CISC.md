# RISC vs CISC

... is a facade.

Have been watching a ton of stuff about CPU architectures lately, and the
consensus seems to be pretty uniform:

1. We don't really have RISC and CISC anymore, atleast in principle.
2. Hardware engineers are VERY secretive.
3. CPUs are ridiculously complex.


### The RISC vs CISC debate

RISC favors simplicity, and a small set of instructions.
Even ops like divide are part of an extension to the ISA.
The [original RISC paper](http://www.cs.ucf.edu/courses/cda5106/summer02/papers/risc-patterson.pdf) by Patterson
dives into the various reasons why RISC is better than CISC. One of the primary motivations was to
not devolve into micro-op madness.

CISC on the other hand, has a large set of instructions, and can do a lot in one instruction.
The [rebuttal to the RISC paper](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=6e91e0c4c7935accb09d811f72ebd5521b585601)
is also a good read.

As time has shown, RISC has won out and CISC seems to be on its way out with Intel losing the plot.
But the reason is not really the "simplicity".

The openness of RISC has led to a lot of competition and innovation of low-powered chips, where
x86 has largely been worked on by Intel and AMD. Power consumption optimization has not really
been the primary goal of x86 chips, but with the rise of mobile and laptop it became a necessity.

Apple's friction with Intel also led them to a RISC based low power chip.

However now, Intel is also able to make low power x86 chips. They figured it out, they were just slow.
Intel also decided to create [self destructing chips](https://www.reddit.com/r/buildapc/comments/1eg8wl0/intel_core_13th14th_gen_issue_megathread/), and managed to self destruct themselves.

So in the end, it was:
1. The wrong objective function to optimize upon.
2. Lack of innovation/competition in the CISC space.


### The facade

Based on the RISC paper, one of the primary reason was to avoid instruction conversion to micro-ops.

However, modern CPUs (be it ARM or x86) end up converting instructions to micro-ops anyway. It seems
to be common industry knowledge that you cannot have a one-size-fits-all reduced instruction set
that is performant.

How is [this](https://stackoverflow.com/questions/78470412/fastest-way-to-do-horizontal-pairwise-rvv-vector-sum-addp-instruction-in-aarch6)
reduced in any way?

This is also expected, since the optimizations that the CPUs are able to perform are heavily
reliant on their silicon, which is constantly being updated to perform more (and weirder?) things faster.

ARM has something called VQRDMULH
Which does the following:
1. Multiplies two vectors
2. Doubles the results
3. Rounds the results
4. Takes the high half
5. Saturates the values

x86 has VPCLMULQDQ, which does polynomial multiplication over [GF(2)](https://en.wikipedia.org/wiki/GF(2)).
For AlderLake-P (which is P for power), this instruction is 1 micro-op!

RISC doesn't shy away from adding complexity, and CISC doesn't shy away from removing it through
simpler micro-ops.

RISC vs CISC today is more of a marketing gimmick, and based on conversations with hardware people,
not something they really care about.
