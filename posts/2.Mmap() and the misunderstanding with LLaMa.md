# mmap() and the misunderstanding with LLaMa

A recent article on [HN](https://news.ycombinator.com/item?id=35393284&ref=emergentmind) went viral claiming the usage of mmap() had caused a memory usage reduction for LLaMa (30B param variant) inference, and now it was possible to do it with 6GB RAM instead of $\ge$ 30GB to load weights and activations. Even quantized to 4-bit integers, it would still require $\ge$ 17GB to fully load the model, but maybe loading it fully isn't necessary? (it is actually, hence the misunderstanding)

### mmap()




