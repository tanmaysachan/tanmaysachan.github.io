# Forward mode Autodiff and Dual Numbers

### Forward autodiff?

While we're familiar with backward mode autodiff due to the almightly backprop, its not the only
way to auto differentiate.

Before autodiff, we also have to consider the standard differentation procedures.

1. Symbolic Differentiation  
Too computationally intensive to obtain a closed form simplified expression.

2. Finite difference method  
The popular $\frac{d}{dx}f(x) = \lim_{x \to 0}\frac{f(x + h) - f(x)}{h}$.
Suffers from numerical instability.

Both are slow for partial derivative computation with respect to many inputs. A more efficient and
stable method for the same is to multiply a chain of jacobians through simple chain rule.

If $x$ and $y$ are vectors, and -  
$y = h(g(f(x)))$

Then through chain rule,  
$\frac{\partial y}{\partial x} = \frac{\partial h(w_2)}{\partial w_2} \frac{\partial g(w_1)}{\partial w_1} \frac{\partial f(w_0)}{\partial x}$

Where $w_0 = x, w_1 = f(w_0), w_2 = g(w_1)$.

Each of these terms in the chain rule equation form a jacobian for multi-variate inputs. Now,
while backprop works by performing a top-down evaluation of this chain rule like so -  

$\frac{\partial y}{\partial x} = \left( \frac{\partial h(w_2)}{\partial w_2} \frac{\partial g(w_1)}{\partial w_1} \right) \frac{\partial f(w_0)}{\partial x}$

Forward prop would instead perform the following evaluation -   

$\frac{\partial y}{\partial x} = \frac{\partial h(w_2)}{\partial w_2} \left( \frac{\partial g(w_1)}{\partial w_1} \frac{\partial f(w_0)}{\partial x} \right)$

Now for complexity analysis, assume $x$ is a vector of size $N$ and $y$ is a vector of size $M$.

* A forward pass would calculate derivatives with respect to one independent input variable in
  one pass.  
* A backward pass would calculate derivatives with respect to one independent output variable in
  one pass.

For a function $f: R^N \rightarrow R^M$, if $N > M$, we can see that backward accumulation is
more efficient. In the other case where $M > N$, a forward accumulation is better. Neural
networks belong to the first class of functions, and hence a backward propagation is required for
efficiency.
