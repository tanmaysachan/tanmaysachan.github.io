# How did transformers get this good?

Back in 2018 when "Attention is all you need" [paper](https://arxiv.org/abs/1706.03762) was published no one had expected
this architecture to take the NLP world by storm. And then it did. And not just NLP, but computer vision, speech processing,
program synthesis and many more.

In 2023, with ChatGPT and GPT-4, transformers have been fully productionized into chatbots/assistants that can automate a variety of tasks.

* What do transformers do exactly?

Generative Pre-trained transformer (GPT) essentially boils down to a probability distribution prediction function. Essentially for some given list of tokens $T = [t_1, t_2, .. t_n]$, it gives us the probability distribution for what $t_{n+1}$ is.

\* GPT is only half of a transformer, consisting only of the decoder part from the original paper. There is also an encoder part in the full architecture which is used for tasks such as machine translation.

* Why now? 

GPT-3, an already strong model had been out since 2020. While people in the NLP space were aware of its strong reasoning capabilities, it was far from a product that an average person could use. While it possessed strong text completion skills, you could not, for example, ask it a question and expect a proper well formatted answer with an intention to help you.

What changed for a next-word-prediction model to become a full fledged AI assistant was **alignment**. And this was accomplished using a technique known as Reinforcement Learning from Human Feedback (RLHF).